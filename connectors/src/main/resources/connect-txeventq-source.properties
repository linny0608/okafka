#
## Kafka Connect for TxEventQ.
##
## Copyright (c) 2023, 2024 Oracle and/or its affiliates.
## Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.
#

#  Licensed to the Apache Software Foundation (ASF) under one or more
#  contributor license agreements. See the NOTICE file distributed with
#  this work for additional information regarding copyright ownership.
#  The ASF licenses this file to You under the Apache License, Version 2.0
#  (the "License"); you may not use this file except in compliance with
#  the License. You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.

name=TxEventQ-source
connector.class=oracle.jdbc.txeventq.kafka.connect.source.TxEventQSourceConnector

# If the transactional event queue has STICKY_DEQUEUE set and running on a database version less than 23.4
# the tasks.max number specified must be equal to the number of event streams (SHARD_NUM) for the queue.
# If the `tasks.max` is not equal to the event streams (SHARD_NUM) dequeue from all event streams will 
# not be performed when using a database with a version less than 23.4.
tasks.max=1

# The maximum number of records to read from the Oracle Transactional Event Queue before writing to Kafka. The minimum is 1 and the default is 250.
txeventq.batch.size=1

# The name of the Kafka topic where the connector writes all records that were read from the JMS broker.
# Note: This property will need to be updated before the Source Connector can connect.
kafka.topic=<Kafka topic>

# This property will specify whether to use the built in schema for JMS Messages. The JMS messages types that
# are supported are BytesMessage, TextMessage, and MapMessage.
# The default value for this property is false.
use.schema.for.jms.msgs=false

# Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka.
# This controls the format of the keys in messages written to or read from Kafka, and since this is independent
# of connectors it allows any connector to work with any serialization format. Some common
# Kafka Connect Converters are: org.apache.kafka.connect.json.JsonConverter, org.apache.kafka.connect.storage.StringConverter,
# and org.apache.kafka.connect.converters.ByteArrayConverter
# Note: If the connector is processing JMS type messages and the `use.schema.for.jms.msgs` configuration property described above
# is set to true these messages will be set as structured data in JSON format. As a result of this the 
# org.apache.kafka.connect.json.JsonConverter should be used.
key.converter=org.apache.kafka.connect.storage.StringConverter

# This configuration property determines whether the schema of the key is included with the data when it is serialized.
# Setting to false (the default) excludes the schema if a schema is available, resulting in a smaller payload. However,
# setting the property to true will include the schema with the data when it is serialized.
# Set property to false:
#   -- If you are using a JSON converter and you don't need to include the schema information in your messages.
#   -- To reduce payload overhead, especially if your downstream applications don't need schema information.
#
# Set property to true:
#   -- If you are using a schema-aware converter.
#   -- If you need to ensure that each record has the correct structure and schema evolution is enable.
#   -- If you need to store schemas with JSON messages for interoperability with certain 3rd-party sink connector.
key.converter.schemas.enable=false

# Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka.
# This controls the format of the values in messages written to or read from Kafka, and since this is independent
# of connectors it allows any connector to work with any serialization format.
# Kafka Connect Converters are: org.apache.kafka.connect.json.JsonConverter, org.apache.kafka.connect.storage.StringConverter,
# and org.apache.kafka.connect.converters.ByteArrayConverter
# Note: If the connector is processing JMS type messages and the `use.schema.for.jms.msgs` configuration property described above
# is set to true these messages will be set as structured data in JSON format. As a result of this the 
# org.apache.kafka.connect.json.JsonConverter should be used.
value.converter=org.apache.kafka.connect.converters.ByteArrayConverter

# This configuration property determines whether the schema of the value is included with the data when it is serialized.
# Setting to false (the default) excludes the schema if a schema is available, resulting in a smaller payload. However,
# setting the property to true will include the schema with the data when it is serialized.
# Set property to false:
#   -- If you are using a JSON converter and you don't need to include the schema information in your messages.
#   -- To reduce payload overhead, especially if your downstream applications don't need schema information.
#
# Set property to true:
#   -- If you are using a schema-aware converter.
#   -- If you need to ensure that each record has the correct structure and schema evolution is enable.
#   -- If you need to store schemas with JSON messages for interoperability with certain 3rd-party sink connector.
value.converter.schemas.enable=false

# Indicate the directory location of where the Oracle wallet is placed i.e. C:/tmp/wallet.
# The cwallet.sso, ewallet.p12, and tnsnames.ora files should be placed into this directory.
# Oracle Wallet provides a simple and easy method to manage database credentials across multiple domains.
# We will be using the Oracle TNS (Transport Network Substrate) administrative file to hide the details
# of the database connection string (host name, port number, and service name) from the datasource definition
# and instead us an alias.
# Note: This property will need to be updated before the Source Connector can connect.
wallet.path=<wallet directory>

# The TNS alias name for the database to connect to stored in the tnsnames.ora.
# An Oracle Wallet must be created and will be used to connect to the database.
# Note: This property will need to be updated before the Source Connector can connect.
db_tns_alias=<tns alias>

# Indicate the directory location of the where the tnsnames.ora location is located i.e C:/tmp/tnsnames.
# The entry in the tnsnames.ora should have the following format: 
# <aliasname> = (DESCRIPTION =(ADDRESS_LIST =(ADDRESS = (PROTOCOL = TCP)(Host = <hostname>)(Port = <port>)))(CONNECT_DATA =(SERVICE_NAME = <service_name>)))
# Note: This property will need to be updated before the Source Connector can connect.
tnsnames.path=<tnsnames.ora directory>

# The TxEventQ to pull data from to put into the specified Kafka topic.
# Note: This property will need to be updated before the Source Connector can connect.
txeventq.queue.name=<txEventQ queue name>

# The subscriber for the TxEventQ that data will be pulled from to put into the specified Kafka topic.
# Note: This property will need to be updated before the Source Connector can connect.
txeventq.subscriber=<txEventQ subscriber>

# List of Kafka brokers used for bootstrapping
# format: host1:port1,host2:port2 ...
# Note: This property will need to be updated before the Source Connector can connect.
bootstrap.servers=<broker i.e localhost:9092>

# Indicates the amount of time the connector should wait for the prior batch of messages to be sent to Kafka before
# a new poll request is made.
# Note: The time specified here should be less than the time defined for the task.shutdown.graceful.timeout.ms property
# since this is the time used by Kafka to determine the amount of time to wait for the tasks to shutdown gracefully. 
source.max.poll.blocked.time.ms=<time in milliseconds, default is 2000>

# This property will specify whether the messages from a TxEventQ shard will be placed into the respective Kafka partition.
# If this property is set to true all the messages from shard 2 will be sent to Kafka partition 1, messages from shard 4 will be
# sent to Kafka partition 2, etc. 
# If ordering within the shards need to be maintained when sent to the Kafka topic this property will
# need to be set to true and the TxEventQ will need to be created with 'STICKY_DEQUEUE' queue parameter set to 1. 
# If this property is set to true the number of Kafka partitions for the topic will need to be equal or greater 
# than the number of shards for the TxEventQ.
# If this property is set to false messages will be sent to a Kafka partition based on the message key or a 
# round-robin approach if no key is provided.
# The default value for this property is false.
txeventq.map.shard.to.kafka_partition=false
