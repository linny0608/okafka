#
## Kafka Connect for TxEventQ.
##
## Copyright (c) 2023, 2024 Oracle and/or its affiliates.
## Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.
#

#  Licensed to the Apache Software Foundation (ASF) under one or more
#  contributor license agreements. See the NOTICE file distributed with
#  this work for additional information regarding copyright ownership.
#  The ASF licenses this file to You under the Apache License, Version 2.0
#  (the "License"); you may not use this file except in compliance with
#  the License. You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.

name=TxEventQ-sink
connector.class=oracle.jdbc.txeventq.kafka.connect.sink.TxEventQSinkConnector

# Maximum number of tasks to use for this connector.
tasks.max=1

# The Kafka topic to read the data from.
# Note: This property will need to be updated before the Sink Connector can connect.
topics=<Kafka topic>

# Indicate the directory location of where the Oracle wallet is place i.e. C:/tmp/wallet.
# The cwallet.sso and ewallet.p12 files should be placed into this directory.
# Oracle Wallet provides a simple and easy method to manage database credentials across multiple domains.
# We will be using the Oracle TNS (Transport Network Substrate) administrative file to hide the details
# of the database connection string (host name, port number, and service name) from the datasource definition
# and instead us an alias.
# Note: This property will need to be updated before the Sink Connector can connect.
wallet.path=<wallet directory>

# Indicate the directory location of the where the tnsnames.ora location is located i.e C:/tmp/tnsnames.
# The entry in the tnsnames.ora should have the following format: 
# <aliasname> = (DESCRIPTION =(ADDRESS_LIST =(ADDRESS = (PROTOCOL = TCP)(Host = <hostname>)(Port = <port>)))(CONNECT_DATA =(SERVICE_NAME = <service_name>)))
# Note: This property will need to be updated before the Sink Connector can connect.
tnsnames.path=<tnsnames.ora directory>

# The TNS alias name for the database to connect to stored in the tnsnames.ora.
# An Oracle Wallet must be created and will be used to connect to the database.
# Note: This property will need to be updated before the Sink Connector can connect.
db_tns_alias=<tns alias>

# The TxEventQ to put the Kafka data into.
# Note: This property will need to be updated before the Sink Connector can connect.
txeventq.queue.name=<txEventQ queue name>

# The name of the schema for the txEventQ queue specified in the txeventq.queue.name property.
# Note: This property will need to be updated to ensure exactly-once delivery.
txeventq.queue.schema=<schema for the txEventQ queue>

# Indicates whether the JMS Bytes message will process Header information from Kafka into TxEventQ.
# Note: The default value for this property is False and Header information from Kafka will be processed into TxEventQ.
txeventq.jms.bytes.include.kafka.headers=<true or false; default is false>

# If enabled, metadata from the Kafka record is forwarded to TxEventQ as part of the Header information stored in the message payload. 
# This includes the record's topic, partition, offset, and timestamp. The topic name is applied as a String property named KAFKA_TOPIC,
# the partition value is applied as an Int property named KAFKA_PARTITION, the offset value is applied as a Long property named KAFKA_OFFSET,
# and the timestamp value is applied as a Long property named KAFKA_TIMESTAMP.
# Note: The default value for this property is False and the Kafka metadata will not be processed into TxEventQ as the header in the message payload.
txeventq.jms.bytes.include.kafka.metadata=<true or false; default is false>

# List of Kafka brokers used for bootstrapping
# format: host1:port1,host2:port2 ...
# Note: This property will need to be updated before the Sink Connector can connect.
bootstrap.servers=<broker i.e localhost:9092>

# Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka.
# This controls the format of the keys in messages written to or read from Kafka.
# key.converter=org.apache.kafka.connect.converters.ByteArrayConverter
key.converter=org.apache.kafka.connect.storage.StringConverter
# key.converter=org.apache.kafka.connect.json.JsonConverter


# Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka.
# This controls the format of the values in messages written to or read from Kafka.
# value.converter=org.apache.kafka.connect.converters.ByteArrayConverter
value.converter=org.apache.kafka.connect.storage.StringConverter
# value.converter=org.apache.kafka.connect.json.JsonConverter